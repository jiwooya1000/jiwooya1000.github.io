---
title: Reference-free Monolitic Odds Ratio Preference Optimization (ORPO)
author: jiwoo
date: 2024-03-03
categories: [NLP, Language Model Alignment]
tags: [üåüPublications, üî•NLP, üî•LLM, üî•Alignment]
pin: true
math: true
mermaid: true
# image:
#   path: /
#   width: 800
#   height: 500
#   alt: 
---

Our paper, **ORPO: Monolithic Odds Ratio Preference Optimization without Reference Model** with <a class="link" style="color: var(--text-muted-color)" href="https://nlee-208.github.io/">Noah Lee</a>, will be uploaded to *Arxiv*! Our best models, ü§ó **Mistral-<tt>ORPO</tt>-$\alpha$ (7B)** and ü§ó **Mistral-<tt>ORPO</tt>-$\beta$ (7B)**, surpasses or is on par with the state-of-the-art instruction-following large language models (in <a class="link" style="color: var(--hyperlink-color)" href="https://github.com/tatsu-lab/alpaca_eval">AlpacaEval</a> and <a class="link" style="color: var(--hyperlink-color)" href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge">MT-Bench</a>), including Zephyr $\beta$ (7B), TULU-2-DPO (13B), and Llama-2-Chat (13B), by fine-tuning pre-trained language models with <tt>ORPO</tt> on single-turn conversation dataset only, *without two separate supervised fine-tuning & alignment phases*. In detail, ü§ó **Mistral-<tt>ORPO</tt>-$\alpha$ (7B)** was trained on the <a class="link" style="color: var(--hyperlink-color)" href="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized">UltraFeedback</a>, and ü§ó **Mistral-<tt>ORPO</tt>-$\beta$ (7B)** was trained on the <a class="link" style="color: var(--hyperlink-color)" href="https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned">cleaned version of UltraFeedback</a> by <a class="link" style="color: var(--hyperlink-color)" href="https://huggingface.co/argilla">Argilla</a>. Find our checkpoints by clicking the model name next to ü§ó in the table. Also check the detailed evaluation results on [AlpacaEval](#alpacaeval), [MT-Bench](#mt-bench), [IFEval](#ifeval), and <a class="link" style="color: var(--hyperlink-color)" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=kaist-ai%2Fmistral-orpo-beta">Open-LLM-Leaderboard</a>!

<!-- **<a class="link" style="color: var(--hyperlink-color)" href="https://openreview.net/forum?id=XNzfEFbEJB3">ORPO: Monolithic Odds Ratio Preference Optimization without Reference Model</a>** -->


|Model Name|Size|Align|Train Set<br>(Single-turn)|Train Set<br>(Multi-turn)|MT-Bench|AlpacaEval 1.0|AlpacaEval 2.0|
|:--------|:--------------:|:--------------:|:--------------:|:--------------:|:-------------------:|:------------:|:------------:|
|ü§ó <a class="link" style="color: var(--text-color)" href="https://huggingface.co/kaist-ai/mistral-orpo-alpha">**Mistral-<tt>ORPO</tt>-$\alpha$**</a>|7B|<tt>ORPO</tt>|‚úÖ|‚ùå|7.23|87.92|<u>11.33</u>|
|ü§ó <a class="link" style="color: var(--text-color)" href="https://huggingface.co/kaist-ai/mistral-orpo-beta">**Mistral-<tt>ORPO</tt>-$\beta$**</a>|7B|<tt>ORPO</tt>|‚úÖ|‚ùå|<u>7.32</u>|**91.41**|**12.20**|
|ü§ó Zephyr ($\beta$) |7B|DPO|‚úÖ|‚úÖ|**7.34**|<u>90.60</u>|10.99|
|ü§ó TULU-2-DPO |13B|DPO|‚úÖ|‚úÖ|7.00|89.5|10.12|
|ü§ó Llama-2-Chat |7B|RLHF|‚úÖ|‚ùì|6.27|71.37|4.96|
|ü§ó Llama-2-Chat |13B|RLHF|‚úÖ|‚ùì|6.65|81.09|7.70|

<!-- |ü§ó Llama-2-Chat (70B)|RLHF|‚úÖ|‚ùì|6.86|92.66|13.87| -->

<!-- 
|Model Name|Train Set<br>(Single-turn)|Train Set<br>(Multi-turn)|MT-Bench<br>(GPT-4)|MT-Bench<br>(Gemini)|AlpacaEval 1.0|AlpacaEval 2.0|
|:--------|:--------------:|:--------------:|:--------------:|:-------------------:|:------------:|:------------:|
|ü§ó **<tt>ORPO</tt>-Mistral (7B)**|‚úÖ|‚ùå|7.32|**7.60**|**91.41**|**12.20**|
|ü§ó Zephyr ($\beta$) (7B)|‚úÖ|‚úÖ|**7.34**|7.40|90.60|10.99|
|ü§ó Llama-2-Chat (13B)|‚úÖ|‚ùì|6.65|7.37|81.09|7.70| -->

<!-- |ü§ó **<tt>ORPO</tt>-Llama-2 (7B)**|0|0|0|0| -->

&nbsp;

## **`Methods`**


In Section 3 of our paper, we study the role of SFT in the context of preference alignment and show that the negative log-likelihood (NLL) loss in SFT simultaneously encourages the log probabilities of both the chosen and rejected responses. Meanwhile, inspired by the intuitions behind <a class="link" style="color: var(--hyperlink-color)" href="https://arxiv.org/abs/2305.18290">DPO</a> and <a class="link" style="color: var(--hyperlink-color)" href="https://arxiv.org/abs/1908.04319">Unlikelihood training</a>, we designed a modified NLL loss which leads to stronger adaptation to the chosen responses and minor penalty to the rejected responses, thereby questioning the necessity of separated alignment scheme.
<figure>
  <img class="png" src="/assets/img/posts/ORPO_main.drawio.png" alt="Description of the image">
  <figcaption><b>Figure 1.</b> General diagram of comparing crucial components in RLHF, DPO, and <tt>ORPO</tt>. <tt>ORPO</tt> effectively handles preference learning during SFT with penalizing the rejected response tokens and leading stronger adaptation to the chosen response tokens.</figcaption>
</figure>


From this background, we propose <tt>ORPO</tt> which penalizes the model from assigning high probabilities to the rejected responses in average **during supervised fine-tuning (SFT)**. Specifically, $L_{SFT}$ follows conventional NLL loss in fine-tuning language models with next token prediction, and $L_{OR}$ is a log odds ratio of the chosen responses' logits over the rejected ones.

$$\mathcal{L}_{ORPO} = -\mathbb{E}_{(x, y_w, y_l)}\left[ \mathcal{L}_{SFT} + \lambda \cdot \mathcal{L}_{OR} \right]$$

$$\mathcal{L}_{OR} = \log \sigma \left( \log \frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)} \right)$$

&nbsp;



## **`Why Odds Ratio?`**

While the most of preference alignment algorithms including DPO are designed with the log probability ratio, we claim that the properties of odds ratio is more adequate in the monolithic situation. In detail, $1-p$ as the denominator of the odds amplifies the odds when the assigned probability $p$ gets larger, resulting in the odds ratio between two probabilities, $p_1$ and $p_2$, to be larger in odds ratio than that of probability ratio. 

$$
\textbf{odds}_\theta(y|x) = \frac{P_\theta(y|x)}{1 - P_\theta(y|x)}
$$

This is a desired property in contrasting two logits in monolithic setting, since there does not exist a reference model which can keep the center point. If the margin of logits get too large, it leads to degeneration issue, as shown in the ablation study from our paper. To see how the margin between the chosen and the rejected responses change during training, please check the Wandb reports: <a class="link" style="color: var(--hyperlink-color)" href="https://wandb.ai/jiwooya1000/PREF/reports/Mistral-ORPO-7B-Training-Log--Vmlldzo3MTE1NzE0?accessToken=rms6o4mg5vo3feu1bvbpk632m4cspe19l0u1p4he3othx5bgean82chn9neiile6">**Mistral-<tt>ORPO</tt>-$\alpha$ (7B)**</a> or <a class="link" style="color: var(--hyperlink-color)" href="https://wandb.ai/jiwooya1000/PREF/reports/Mistral-ORPO-7B-Training-Log--Vmlldzo3MTE3MzMy?accessToken=dij4qbp6dcrofsanzbgobjsne9el8a2zkly2u5z82rxisd4wiwv1rhp0s2dub11e">**Mistral-<tt>ORPO</tt>-$\beta$ (7B)**</a>.

&nbsp;


## **`AlpacaEval`**

We report the single-turn instruction-following skills of two models through AlpacaEval. In AlpacaEval 1.0, ü§ó **Mistral-<tt>ORPO</tt>-$\alpha$ (7B)** and ü§ó **Mistral-<tt>ORPO</tt>-$\beta$ (7B)** scores **87.92$\%$** and **91.41$\%$**, exceeding Llama-2-Chat models with the size of 7B and 13B and Zephyr $\alpha$ and Zephyr $\beta$. Furthermore, in AlpacaEval 2.0, ü§ó **Mistral-<tt>ORPO</tt>-$\alpha$ (7B)** and ü§ó **Mistral-<tt>ORPO</tt>-$\beta$ (7B)** scores **11.33$\%$** and **12.20$\%$**. We explicitly compare the score against Llama-2-Chat models and Zephyr models, the checkpoints that are trained with RLHF and DPO, respectively. It is noteworthy that while those models were trained with significantly more data (e.g., Zephyr was trained on 270k conversations in total and ü§ó **Mistral-<tt>ORPO</tt>** series were trained on 61k conversations), **<tt>ORPO</tt>** surpasses corresponding checkpoints.

<figure>
  <img class="png" src="/assets/img/posts/alpaca_blog.png" alt="Description of the image">
  <figcaption><b>Figure 2.</b> AlpacaEval\(\text{}_{2.0}\) score of <b>Llama-2-\(\texttt{ORPO}\) (7B)</b>, ü§ó <b>Mistral-\(\texttt{ORPO}\)-\(\alpha\) (7B)</b>, and ü§ó <b>Mistral-\(\texttt{ORPO}\)-\(\beta\) (7B)</b> compared to RLHF and DPO models. They surpass RLHF and DPO-tuned models, respectively.</figcaption>
</figure>

&nbsp;

## **`MT-Bench`**

We report the comprehensive instruction-following skills in both the single-turn and multi-turn conversation through MT-Bench. Our best models, ü§ó **Mistral-<tt>ORPO</tt>-$\alpha$ (7B)** and ü§ó **Mistral-<tt>ORPO</tt>-$\beta$ (7B)**, achieved **7.23** and **7.32** in MT-Bench, even though they were trained on the 61k instances of single-turn conversation dataset (UltraFeedback) alone. Moreover, we implemented MT-Bench with Gemini-Pro as an evaluator, and ü§ó **<tt>ORPO</tt>-Mistral (7B)** outperforms GPT-3.5-turbo (7.23), Claude-v1 (7.36), and Zephyr $\beta$ (7.40) by getting **7.60**.

<div style="display: flex; justify-content: center; align-items: center;">
  <embed src="/assets/img/posts/mtbench_blog.html" style="width: 75rem; height: 35rem;" />
</div>

&nbsp;

## **`IFEval`**

Finally, we assess the strict instruction-following skills of our models with <a class="link" style="color: var(--hyperlink-color)" href="https://arxiv.org/abs/2311.07911">IFEval</a>. The scores are measured with <a class="link" href="https://github.com/EleutherAI/lm-evaluation-harness">EleutherAI/lm-evaluation-harness</a> by applying the chat template. The scores for Llama-2-Chat (70B), Zephyr-Œ≤ (7B), Mixtral-8X7B-Instruct-v0.1, GPT-3.5-Turbo, and GPT-4 are originally reported in <a class="link" href="https://twitter.com/wiskojo/status/1739767758462877823">this tweet</a>. The highest scores for non-proprietary models are bolded.

| **Model Type**     | **Prompt-Strict** | **Prompt-Loose** | **Inst-Strict** | **Inst-Loose** |
|--------------------|:-----------------:|:----------------:|:---------------:|----------------|
| **Llama-2-Chat (70B)** |       0.4436      |      0.5342      |      0.5468     |     0.6319     |
| **Zephyr-Œ≤ (7B)** |       0.4233      |      0.4547      |      0.5492     |     0.5767     |
| **Mixtral-8X7B-Instruct-v0.1** |       0.5213      |      **0.5712**      |      0.6343     |     **0.6823**     |
| **Mistral-ORPO-‚ç∫ (7B)** |       0.5009      |      0.5083      |      0.5995     |     0.6163     |
| **Mistral-ORPO-Œ≤ (7B)** |       **0.5287**      |      0.5564      |      **0.6355**     |     0.6619     |
| **GPT-3.5-Turbo** |       0.5767      |      0.6414      |      0.6835     |     0.7338     |
| **GPT-4** |       0.7542      |      0.8115      |      0.8261     |     0.8681     |

<!-- <embed src="/assets/img/posts/mtbench_blog.html" style="width: 50rem; height: 25rem;" /> -->

<!-- <style>
  #my-embedded-content {
    width: 500rem;
    height: 12rem;
  }
</style> -->

<!-- <div id="my-embedded-content">
  <embed src="/assets/img/posts/mtbench_blog.html" />
</div> -->

<!-- <figure>
  <img class="png" src="/assets/img/posts/mtbench_blog.png" alt="Description of the image">
  <figcaption><b>Figure 3.</b> MT-Bench score distribution of ü§ó <b>\(\texttt{ORPO}\)-Mistral (7B)</b> compared to state-of-the-art instruction-following language models.</figcaption>
</figure> -->


<!-- Inspired by the intuitions behind <a class="link" style="color: var(--hyperlink-color)" href="https://arxiv.org/abs/2305.18290">DPO</a> and <a class="link" style="color: var(--hyperlink-color)" href="https://arxiv.org/abs/1908.04319">Unlikelihood training</a> -->