---
title: Reference-free Monolitic Odds Ratio Preference Optimization (ORPO)
author: jiwoo
date: 2024-03-03
categories: [NLP, Language Model Alignment]
tags: [ðŸŒŸPublications, ðŸ”¥NLP, ðŸ”¥LLM, ðŸ”¥Alignment]
pin: true
math: true
mermaid: true
# image:
#   path: /
#   width: 800
#   height: 500
#   alt: 
---

Our paper, **<a class="link" style="color: var(--hyperlink-color)" href="https://openreview.net/forum?id=XNzfEFbEJB3">ORPO: Monolithic Odds Ratio Preference Optimization without Reference Model</a>** is uploaded to *Arxiv*! Our best model, **ORPO-Mistral (7B)** and **ORPO-Llama-2 (7B)** surpasses the state-of-the-art instruction-following large language models by fine-tuning pre-trained language models with <tt>ORPO</tt> on single-turn conversation <a class="link" style="color: var(--hyperlink-color)" href="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized">UltraFeedback dataset</a> only, including Zephyr ($\alpha$), Zephyr ($\beta$), and Llama-2-Chat (13B) in <a class="link" style="color: var(--hyperlink-color)" href="https://github.com/tatsu-lab/alpaca_eval">AlpacaEval</a> and <a class="link" style="color: var(--hyperlink-color)" href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge">MT-Bench</a> *without two distinct supervised fine-tuning & alignment phases*.

<figure>
  <img class="png" src="/assets/img/posts/orpo_blog.png" alt="Description of the image">
  <figcaption>Figure 1. AlpacaEval\(\text{}_{2.0}\) score of \(\texttt{ORPO}\)-Llama-2 (7B) and \(\texttt{ORPO}\)-Mistral (7B) compared to RLHF and DPO models. They surpass RLHF and DPO-tuned models, respectively.</figcaption>
</figure>

You can find our models, **ORPO-Mistral (7B)** and **ORPO-Llama-2 (7B)** in:
- [X] ðŸ¤— **ORPO-Mistral (7B)**
- [X] ðŸ¤— **ORPO-Llama-2 (7B)**

&nbsp;

<!-- **`Abstract`**

While recently proposed preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence in preference alignment. In this paper, we elaborate on the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference-free monolithic odds ratio preference optimization algorithm, <tt>ORPO</tt>, eliminating the necessity for an additional preference alignment phase. Empirically and theoretically, we demonstrate that the odds ratio serves as a sensible choice for contrasting favored and unfavored styles during SFT. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with <tt>ORPO</tt> on UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters, achieving 66.2%, 81.3%, and 91.41% in AlpacaEval.


&nbsp; -->

**`Methods`**

In Section 3 of our paper, we study the role of SFT in the context of preference alignment and show that the negative log-likelihood (NLL) loss in SFT simultaneously encourages the log probabilities of both the chosen and rejected responses. Meanwhile, inspired by the intuitions behind <a class="link" style="color: var(--hyperlink-color)" href="https://arxiv.org/abs/2305.18290">DPO</a> and <a class="link" style="color: var(--hyperlink-color)" href="https://arxiv.org/abs/1908.04319">Unlikelihood training</a>, we designed a modified NLL loss which leads to stronger adaptation to the chosen responses and minor penalty to the rejected responses, thereby questioning the necessity of separated alignment scheme.

From this background, we propose <tt>ORPO</tt> which penalizes the model from assigning high probabilities to the rejected responses in average **during supervised fine-tuning (SFT)**. 

$$
\mathcal{L}_{ORPO} = -\mathbb{E}_{(x, y_w, y_l)}\left[ \mathcal{L}_{SFT} + \lambda \cdot \mathcal{L}_{Ratio} \right]
$$


<!-- Inspired by the intuitions behind <a class="link" style="color: var(--hyperlink-color)" href="https://arxiv.org/abs/2305.18290">DPO</a> and <a class="link" style="color: var(--hyperlink-color)" href="https://arxiv.org/abs/1908.04319">Unlikelihood training</a> -->