

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>Jiwoo Hong</title>
  <subtitle>Static site</subtitle>
  <updated>2024-03-03T09:16:53-05:00</updated>
  <author>
    <name>Jiwoo Hong</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator>
  <rights> © 2024 Jiwoo Hong </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>Arxiv - Reference-free Monolitic Odds Ratio Preference Optimization (ORPO)</title>
    <link href="http://localhost:4000/posts/eMoviesRework/" rel="alternate" type="text/html" title="Arxiv - Reference-free Monolitic Odds Ratio Preference Optimization (ORPO)" />
    <published>2024-03-03T00:00:00-05:00</published>
  
    <updated>2024-03-03T00:00:00-05:00</updated>
  
    <id>http://localhost:4000/posts/eMoviesRework/</id>
    <content src="http://localhost:4000/posts/eMoviesRework/" />
    <author>
      <name>jiwoo</name>
    </author>

  
    
    <category term="NLP" />
    
    <category term="Language Model Alignment" />
    
  

  
    <summary>
      





      Our paper, ORPO: Monolithic Odds Ratio Preference Optimization without Reference Model is uploaded to Arxiv.org! Our best model, ORPO-Mistral (7B) and ORPO-Llama-2 (7B) surpasses the state-of-the-art instruction-following large language models, including Zephyr ($\alpha$), Zephyr ($\beta$), and Llama-2-Chat (13B) in AlpacaEval and MT-Bench.



 

Abstract

While recently proposed preference ali...
    </summary>
  

  </entry>

  
  <entry>
    <title>EMNLP 2023 - Our paper is accepted to the Findings of EMNLP!</title>
    <link href="http://localhost:4000/posts/emnlp2023/" rel="alternate" type="text/html" title="EMNLP 2023 - Our paper is accepted to the Findings of EMNLP!" />
    <published>2023-12-06T00:00:00-05:00</published>
  
    <updated>2023-12-06T00:00:00-05:00</updated>
  
    <id>http://localhost:4000/posts/emnlp2023/</id>
    <content src="http://localhost:4000/posts/emnlp2023/" />
    <author>
      <name>jiwoo</name>
    </author>

  
    
    <category term="NLP" />
    
    <category term="Journalism AI" />
    
    <category term="EMNLP2023" />
    
  

  
    <summary>
      





      Our paper, Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy is accepted to the Findings of EMNLP 2023!

 

Abstract

We address an important gap in detecting political bias in news articles. Previous works that perform document classification can be influenced by the writing style of each news outlet, leading to overfitting and limited generaliz...
    </summary>
  

  </entry>

  
  <entry>
    <title>Applied Science - Our paper is accepted to the Applied Science (SCIE)!</title>
    <link href="http://localhost:4000/posts/applied-science/" rel="alternate" type="text/html" title="Applied Science - Our paper is accepted to the Applied Science (SCIE)!" />
    <published>2022-05-07T00:00:00-04:00</published>
  
    <updated>2022-05-07T00:00:00-04:00</updated>
  
    <id>http://localhost:4000/posts/applied-science/</id>
    <content src="http://localhost:4000/posts/applied-science/" />
    <author>
      <name>jiwoo</name>
    </author>

  
    
    <category term="Reinforcement Learning" />
    
    <category term="Multi-Agent Reinforcement Learning" />
    
  

  
    <summary>
      





      Our paper, MARL-Based Dual Reward Model on Segmented Actions for Multiple Mobile Robots in Automated Warehouse Environment is accepted to the Applied Science!

 

Abstract

The simple and labor-intensive tasks of workers on the job site are rapidly becoming digital. In the work environment of logistics warehouses and manufacturing plants, moving goods to a designated place is a typical labor-in...
    </summary>
  

  </entry>

</feed>


