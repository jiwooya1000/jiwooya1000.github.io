[
  
  {
    "title": "Reference-free Monolitic Odds Ratio Preference Optimization (ORPO)",
    "url": "/posts/orpo/",
    "categories": "NLP, Language Model Alignment",
    "tags": "ðŸŒŸPublications, ðŸ”¥NLP, ðŸ”¥LLM, ðŸ”¥Alignment",
    "date": "2024-03-03 00:00:00 -0500",
    





    "snippet": "Our paper, ORPO: Monolithic Odds Ratio Preference Optimization without Reference Model is uploaded to Arxiv! Our best model, ORPO-Mistral (7B) and ORPO-Llama-2 (7B) surpasses the state-of-the-art instruction-following large language models by fine-tuning pre-trained language models with ORPO on single-turn conversation UltraFeedback dataset only, including Zephyr ($\\alpha$), Zephyr ($\\beta$), and Llama-2-Chat (13B) in AlpacaEval and MT-Bench without two distinct supervised fine-tuning &amp; alignment phases.    Figure 1. AlpacaEval\\(\\text{}_{2.0}\\) score of \\(\\texttt{ORPO}\\)-Llama-2 (7B) and \\(\\texttt{ORPO}\\)-Mistral (7B) compared to RLHF and DPO models. They surpass RLHF and DPO-tuned models, respectively.You can find our models, ORPO-Mistral (7B) and ORPO-Llama-2 (7B) in:  ðŸ¤— ORPO-Mistral (7B)  ðŸ¤— ORPO-Llama-2 (7B)Â MethodsIn Section 3 of our paper, we study the role of SFT in the context of preference alignment and show that the negative log-likelihood (NLL) loss in SFT simultaneously encourages the log probabilities of both the chosen and rejected responses. Meanwhile, inspired by the intuitions behind DPO and Unlikelihood training, we designed a modified NLL loss which leads to stronger adaptation to the chosen responses and minor penalty to the rejected responses, thereby questioning the necessity of separated alignment scheme.From this background, we propose ORPO which penalizes the model from assigning high probabilities to the rejected responses in average during supervised fine-tuning (SFT).\\[\\mathcal{L}_{ORPO} = -\\mathbb{E}_{(x, y_w, y_l)}\\left[ \\mathcal{L}_{SFT} + \\lambda \\cdot \\mathcal{L}_{Ratio} \\right]\\]"
  },
  
  {
    "title": "Our paper is accepted to the Findings of EMNLP 2023!",
    "url": "/posts/emnlp2023/",
    "categories": "NLP, Journalism AI, EMNLP2023",
    "tags": "ðŸŒŸPublications, ðŸ”¥NLP, Journalism AI, EMNLP2023",
    "date": "2023-12-06 00:00:00 -0500",
    





    "snippet": "Our paper, Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy is accepted to the Findings of EMNLP 2023!Â AbstractWe address an important gap in detecting political bias in news articles. Previous works that perform document classification can be influenced by the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis and human evaluation demonstrate the ability of our model to capture common discourse structures in journalism.Â Main MethodThe proposed architecture primarily utilizes the frozen SBERT (Reimers and Gurevych, 2019) to encode the news article by sentences. Then, it calculates semantic relationships between the sentences through BiLSTM and shallow multi-head attention module, which allows us to understand the hierarchical discourse structure of the document. With this explainable property, our model precisely classify the political bias of the given news article in the low resource setting."
  },
  
  {
    "title": "Our paper is accepted to the Applied Science (SCIE)!",
    "url": "/posts/applied-science/",
    "categories": "Reinforcement Learning, Multi-Agent Reinforcement Learning",
    "tags": "ðŸŒŸPublications, ðŸ”¥Reinforcement Learning, Multi-Agent Reinforcement Learning",
    "date": "2022-05-07 00:00:00 -0400",
    





    "snippet": "Our paper, MARL-Based Dual Reward Model on Segmented Actions for Multiple Mobile Robots in Automated Warehouse Environment is accepted to the Applied Science!Â AbstractThe simple and labor-intensive tasks of workers on the job site are rapidly becoming digital. In the work environment of logistics warehouses and manufacturing plants, moving goods to a designated place is a typical labor-intensive task for workers. These tasks are rapidly undergoing digital transformation by leveraging mobile robots in automated warehouses. In this paper, we studied and tested realistically necessary conditions to operate mobile robots in an automated warehouse. In particular, considering conditions for operating multiple mobile robots in an automated warehouse, we added more complex actions and various routes and proposed a method for improving sparse reward problems when learning paths in a warehouse with reinforcement learning. Multi-Agent Reinforcement Learning (MARL) experiments were conducted with multiple mobile robots in an automated warehouse simulation environment, and it was confirmed that the proposed reward model method makes learning start earlier even there is a sparse reward problem and learning progress was maintained stably. We expect this study to help us understand the actual operation of mobile robots in an automated warehouse further.Â Keywords  Multi-Agent Reinforcement Learning  Sparse Reward  Reward Shaping"
  }
  
]

