[
  
  {
    "title": "Reference-free Monolitic Odds Ratio Preference Optimization (ORPO)",
    "url": "/posts/orpo/",
    "categories": "NLP, Language Model Alignment",
    "tags": "üåüPublications, üî•NLP, üî•LLM, üî•Alignment",
    "date": "2024-03-03 00:00:00 +0900",
    





    "snippet": "Our paper, ORPO: Monolithic Odds Ratio Preference Optimization without Reference Model with Noah Lee, will be uploaded to Arxiv! Our best model, ü§ó ORPO-Mistral (7B), surpasses or is on par with the state-of-the-art instruction-following large language models (in AlpacaEval and MT-Bench), including Zephyr ($\\beta$), TULU-2-DPO (13B), and Llama-2-Chat (13B), by fine-tuning pre-trained language models with ORPO on single-turn conversation UltraFeedback dataset only, without two separate supervised fine-tuning &amp; alignment phases. Find our checkpoint by clicking ü§ó in the table (TBU)!            Model Name      Size      Align      Train Set(Single-turn)      Train Set(Multi-turn)      MT-Bench      AlpacaEval 1.0      AlpacaEval 2.0                  ü§ó ORPO-Mistral      7B      ORPO      ‚úÖ      ‚ùå      7.32      91.41      12.20              ü§ó Zephyr ($\\beta$)      7B      DPO      ‚úÖ      ‚úÖ      7.34      90.60      10.99              ü§ó TULU-2-DPO      13B      DPO      ‚úÖ      ‚úÖ      7.00      89.5      10.12              ü§ó Llama-2-Chat      13B      RLHF      ‚úÖ      ‚ùì      6.65      81.09      7.70      ¬†MethodsIn Section 3 of our paper, we study the role of SFT in the context of preference alignment and show that the negative log-likelihood (NLL) loss in SFT simultaneously encourages the log probabilities of both the chosen and rejected responses. Meanwhile, inspired by the intuitions behind DPO and Unlikelihood training, we designed a modified NLL loss which leads to stronger adaptation to the chosen responses and minor penalty to the rejected responses, thereby questioning the necessity of separated alignment scheme.    Figure 1. General diagram of comparing crucial components in RLHF, DPO, and ORPO. ORPO effectively handles preference learning during SFT with penalizing the rejected response tokens and leading stronger adaptation to the chosen response tokens.From this background, we propose ORPO which penalizes the model from assigning high probabilities to the rejected responses in average during supervised fine-tuning (SFT). Specifically, $L_{SFT}$ follows conventional NLL loss in fine-tuning language models with next token prediction, and $L_{Ratio}$ is a log odds ratio of the chosen responses‚Äô logits over the rejected ones.\\[\\mathcal{L}_{ORPO} = -\\mathbb{E}_{(x, y_w, y_l)}\\left[ \\mathcal{L}_{SFT} + \\lambda \\cdot \\mathcal{L}_{Ratio} \\right]\\]\\[\\mathcal{L}_{Ratio} = \\log \\sigma \\left( \\log \\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)} \\right)\\]¬†Why Odds Ratio?While the most of preference alignment algorithms including DPO are designed with the log probability ratio, we claim that the properties of odds ratio is more adequate in the monolithic situation. In detail, $1-p$ as the denominator of the odds amplifies the odds when the assigned probability $p$ gets larger, resulting in the odds ratio between two probabilities, $p_1$ and $p_2$, to be larger in odds ratio than that of probability ratio.\\[\\textbf{odds}_\\theta(y|x) = \\frac{P_\\theta(y|x)}{1 - P_\\theta(y|x)}\\]This is a desired property in contrasting two logits in monolithic setting, since there does not exist a reference model which can keep the center point. If the margin of logits get too large, it leads to degeneration issue, as shown in the ablation study from our paper.¬†AlpacaEvalWe report the single-turn instruction-following skills of two models through AlpacaEval. In AlpacaEval 1.0, ü§ó ORPO-Llama-2 (7B) and ü§ó ORPO-Mistral (7B) scores 81.26$\\%$ and 91.41$\\%$, exceeding Llama-2-Chat models with the size of 7B and 13B. Furthermore, in AlpacaEval 2.0, ü§ó ORPO-Llama-2- (7B) and ü§ó ORPO-Mistral (7B) scores 9.44$\\%$ and 12.20$\\%$. We explicitly compare the score against Llama-2-Chat models and Zephyr models, the checkpoints that are trained with RLHF and DPO, respectively. It is noteworthy that while those models were trained with significantly more data (e.g., Zephyr was trained on 270k conversations in total and ü§ó ORPO-Mistral (7B) was trained on 61k conversations), ORPO surpasses corresponding checkpoints.    Figure 2. AlpacaEval\\(\\text{}_{2.0}\\) score of ü§ó \\(\\texttt{ORPO}\\)-Llama-2 (7B) and ü§ó \\(\\texttt{ORPO}\\)-Mistral (7B) compared to RLHF and DPO models. They surpass RLHF and DPO-tuned models, respectively.¬†MT-BenchWe report the comprehensive instruction-following skills in both the single-turn and multi-turn conversation through MT-Bench. Our best model, ü§ó ORPO-Mistral (7B), achieved 7.32 in MT-Bench (GPT-4), even though they were trained on the 61k instances of single-turn conversation dataset (UltraFeedback) alone. Moreover, we implemented MT-Bench with Gemini-Pro as an evaluator, and ü§ó ORPO-Mistral (7B) outperforms GPT-3.5-turbo (7.23), Claude-v1 (7.36), and Zephyr ($\\beta$) (7.40) by getting 7.60.  "
  },
  
  {
    "title": "Our paper is accepted to the Findings of EMNLP 2023!",
    "url": "/posts/emnlp2023/",
    "categories": "NLP, Journalism AI, EMNLP2023",
    "tags": "üåüPublications, üî•NLP, Journalism AI, EMNLP2023",
    "date": "2023-12-06 00:00:00 +0900",
    





    "snippet": "Our paper, Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy is accepted to the Findings of EMNLP 2023!¬†AbstractWe address an important gap in detecting political bias in news articles. Previous works that perform document classification can be influenced by the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis and human evaluation demonstrate the ability of our model to capture common discourse structures in journalism.¬†Main MethodThe proposed architecture primarily utilizes the frozen SBERT (Reimers and Gurevych, 2019) to encode the news article by sentences. Then, it calculates semantic relationships between the sentences through BiLSTM and shallow multi-head attention module, which allows us to understand the hierarchical discourse structure of the document. With this explainable property, our model precisely classify the political bias of the given news article in the low resource setting."
  },
  
  {
    "title": "Our paper is accepted to the Applied Science (SCIE)!",
    "url": "/posts/applied-science/",
    "categories": "Reinforcement Learning, Multi-Agent Reinforcement Learning",
    "tags": "üåüPublications, üî•Reinforcement Learning, Multi-Agent Reinforcement Learning",
    "date": "2022-05-07 00:00:00 +0900",
    





    "snippet": "Our paper, MARL-Based Dual Reward Model on Segmented Actions for Multiple Mobile Robots in Automated Warehouse Environment is accepted to the Applied Science!¬†AbstractThe simple and labor-intensive tasks of workers on the job site are rapidly becoming digital. In the work environment of logistics warehouses and manufacturing plants, moving goods to a designated place is a typical labor-intensive task for workers. These tasks are rapidly undergoing digital transformation by leveraging mobile robots in automated warehouses. In this paper, we studied and tested realistically necessary conditions to operate mobile robots in an automated warehouse. In particular, considering conditions for operating multiple mobile robots in an automated warehouse, we added more complex actions and various routes and proposed a method for improving sparse reward problems when learning paths in a warehouse with reinforcement learning. Multi-Agent Reinforcement Learning (MARL) experiments were conducted with multiple mobile robots in an automated warehouse simulation environment, and it was confirmed that the proposed reward model method makes learning start earlier even there is a sparse reward problem and learning progress was maintained stably. We expect this study to help us understand the actual operation of mobile robots in an automated warehouse further.¬†Keywords  Multi-Agent Reinforcement Learning  Sparse Reward  Reward Shaping"
  }
  
]

