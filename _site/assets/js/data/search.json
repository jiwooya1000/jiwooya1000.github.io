[
  
  {
    "title": "Arxiv - Reference-free Monolitic Odds Ratio Preference Optimization (ORPO)",
    "url": "/posts/eMoviesRework/",
    "categories": "NLP, Language Model Alignment",
    "tags": "ðŸ”¥Publications, NLP, LLM, Alignment",
    "date": "2024-03-03 00:00:00 -0500",
    





    "snippet": "Our paper, ORPO: Monolithic Odds Ratio Preference Optimization without Reference Model is uploaded to Arxiv.org! Our best model, ORPO-Mistral (7B) and ORPO-Llama-2 (7B) surpasses the state-of-the-art instruction-following large language models, including Zephyr ($\\alpha$), Zephyr ($\\beta$), and Llama-2-Chat (13B) in AlpacaEval and MT-Bench.Â AbstractWhile recently proposed preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence in preference alignment. In this paper, we elaborate on the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. Empirically and theoretically, we demonstrate that the odds ratio serves as a sensible choice for contrasting favored and unfavored styles during SFT. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters, achieving 66.2%, 81.3%, and 87.94% in AlpacaEval.Â Keywords  Multi-Agent Reinforcement Learning  Sparse Reward  Reward Shaping"
  },
  
  {
    "title": "EMNLP 2023 - Our paper is accepted to the Findings of EMNLP!",
    "url": "/posts/emnlp2023/",
    "categories": "NLP, Journalism AI, EMNLP2023",
    "tags": "ðŸ”¥Publications, NLP, Journalism AI, EMNLP2023",
    "date": "2023-12-06 00:00:00 -0500",
    





    "snippet": "Our paper, Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy is accepted to the Findings of EMNLP 2023!Â AbstractWe address an important gap in detecting political bias in news articles. Previous works that perform document classification can be influenced by the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis and human evaluation demonstrate the ability of our model to capture common discourse structures in journalism.Â Main MethodThe proposed architecture primarily utilizes the frozen SBERT (Reimers and Gurevych, 2019) to encode the news article by sentences. Then, it calculates semantic relationships between the sentences through BiLSTM and shallow multi-head attention module, which allows us to understand the hierarchical discourse structure of the document. With this explainable property, our model precisely classify the political bias of the given news article in the low resource setting.  Split the bill x-ways  Tip percentages may be adjusted  Settings are stored after app restarts and/or is closed  Settings can be reset back to defaults  Features dark/light mode on settings screen"
  },
  
  {
    "title": "Applied Science - Our paper is accepted to the Applied Science (SCIE)!",
    "url": "/posts/applied-science/",
    "categories": "Reinforcement Learning, Multi-Agent Reinforcement Learning",
    "tags": "ðŸ”¥Publications, Reinforcement Learning, Multi-Agent Reinforcement Learning",
    "date": "2022-05-07 00:00:00 -0400",
    





    "snippet": "Our paper, MARL-Based Dual Reward Model on Segmented Actions for Multiple Mobile Robots in Automated Warehouse Environment is accepted to the Applied Science!Â AbstractThe simple and labor-intensive tasks of workers on the job site are rapidly becoming digital. In the work environment of logistics warehouses and manufacturing plants, moving goods to a designated place is a typical labor-intensive task for workers. These tasks are rapidly undergoing digital transformation by leveraging mobile robots in automated warehouses. In this paper, we studied and tested realistically necessary conditions to operate mobile robots in an automated warehouse. In particular, considering conditions for operating multiple mobile robots in an automated warehouse, we added more complex actions and various routes and proposed a method for improving sparse reward problems when learning paths in a warehouse with reinforcement learning. Multi-Agent Reinforcement Learning (MARL) experiments were conducted with multiple mobile robots in an automated warehouse simulation environment, and it was confirmed that the proposed reward model method makes learning start earlier even there is a sparse reward problem and learning progress was maintained stably. We expect this study to help us understand the actual operation of mobile robots in an automated warehouse further.Â Keywords  Multi-Agent Reinforcement Learning  Sparse Reward  Reward Shaping"
  }
  
]

